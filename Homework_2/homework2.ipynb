{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x, y):\n",
    "    X_t = x.T\n",
    "    X_y = X_t.dot(y) # x is a column vector so it needs to be transformed\n",
    "    X_XT = X_t.dot(x)\n",
    "    w = np.linalg.solve(X_XT, X_y) #(X_XT dot X_y)^-1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, weight, b):\n",
    "    y_hat = x.dot(weight) + b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_hat, y):\n",
    "    n = len(y)\n",
    "    mse = (1/(2 * n)) * np.sum(np.square(y_hat-y))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(w, y, alpha = 0.01):\n",
    "    n = len(y)\n",
    "    wt_w = (w).dot(w.T) # (w^T)w\n",
    "    reg = (1/(2 * n) * alpha) * wt_w # (alpha/2n) * ((w^T)w) \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_mean_square_error(y, y_hat, reg):\n",
    "    n = len(y)\n",
    "    mse = mean_square_error(y_hat, y)\n",
    "    return mse + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a training group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.8\n",
    "def random_train_test_split(dataset, train_perc = 0.8):\n",
    "    perc_of_dataset = dataset.shape[1] * train_perc\n",
    "    numpy.random.shuffle(dataset)\n",
    "\n",
    "    train = dataset_arrx[:perc_of_dataset,:] \n",
    "    test = dataset_arr[perc_of_dataset:,:]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a validation group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                             V\n",
    "#                                                |------------#---------#\n",
    "#                                                |            |         |\n",
    "#                                                | Validation | Testing |\n",
    "#                                                |     Set    |   Set   |\n",
    "#                                                |------------#---------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.5 for a 50/50 split of the test set \n",
    "def random_test_validation_split(parameter, labels, train_perc = 0.8):\n",
    "    dataset_row_size, dataset_col_size = parameter.shape\n",
    "    validation_set_size = int(dataset_row_size * train_perc)\n",
    "    test_set_size = int(dataset_row_size - validation_set_size)\n",
    "\n",
    "    #get random indices for both validation and test sets\n",
    "    indices = np.random.permutation(parameter.shape[0])\n",
    "    validation_p_index = indices[:validation_set_size]\n",
    "    test_p_index = indices[test_set_size:]\n",
    "\n",
    "    #validation and test labels\n",
    "    validation_l = labels[validation_p_index]\n",
    "    test_l = labels[test_p_index]\n",
    "\n",
    "    #validation and test parameters \n",
    "    validation_p = parameter[validation_p_index]\n",
    "    test_p = parameter[test_p_index]\n",
    "\n",
    "    return validation_p, validation_l, test_p, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the input \n",
    "#y is the real result\n",
    "#w is the weight \n",
    "#epsilon is the learning rate\n",
    "#alpha is the regularized term\n",
    "#reg_bool if you wish to use the regularized term\n",
    "def gradient_descent(x, y, b, w, epsilon, reg_bool):\n",
    "    X_t = x.T\n",
    "    n = len(y)\n",
    "    y_hat = model(x, w, b)\n",
    "\n",
    "    #taking derivative \n",
    "    derivative_of_w = (1/n) * np.sum(X_t.dot(y_hat - y))\n",
    "    derivative_of_b = (1/n) * np.sum(y_hat - y)\n",
    "\n",
    "    # if regularized term is wanted then add that to the derivative with respect to w\n",
    "    if reg_bool == True:\n",
    "        derivative_of_w += ((alpha/n) * w)\n",
    "\n",
    "    #this is gradient descent\n",
    "    w_new = w - (epsilon * derivative_of_w)\n",
    "    b_new = b - (epsilon * derivative_of_b)\n",
    "\n",
    "    return w_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ripped from https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(b))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_that_beh(history: dict):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history[\"epoch\"], history[\"cost\"] , 'g', label='loss over epoch')\n",
    "    plt.title('cost to epoch ')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history[\"epoch\"], history[\"learning_rate\"], 'b', label='learning rate over epoch')\n",
    "    plt.title('learning rate to epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(parameters, labels, size_of_batch):\n",
    "    l = np.expand_dims(a=labels, axis=-1)\n",
    "    index = np.random.randint(0, len(parameters), (size_of_batch, parameters.shape[1])) # random sample\n",
    "    mini_batch_x = np.take(parameters, index)\n",
    "    mini_batch_y = np.take(labels, index)\n",
    "    \n",
    "\n",
    "    return mini_batch_x, mini_batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, w, learning_rate: float, num_of_epochs: int, size_of_batch: int, size_of_dataset: int, reg_bool: bool = True):\n",
    "    past_cost = 10000000\n",
    "    current_cost = 0\n",
    "    history = {\n",
    "        \"epoch\": -1,\n",
    "        \"weight\": -1,\n",
    "        \"bias\": -1,\n",
    "        \"cost\": -1,\n",
    "        \"batch_size\": math.ceil(size_of_dataset/size_of_batch),\n",
    "        \"learning rate\": -1\n",
    "    }\n",
    "\n",
    "    epochs_since_improved = 0\n",
    "\n",
    "    for e in range(num_of_epochs - 1):\n",
    "        shuffled_x, shuffled_y = unison_shuffled_copies(x, y)\n",
    "\n",
    "        for b in range(math.ceil(size_of_dataset/size_of_batch) - 1):\n",
    "            mini_batch_x, mini_batch_y = batch_iterator(x, y, size_of_batch)\n",
    "            y_hat = model(mini_batch_x, w, b)\n",
    "\n",
    "            if reg_bool == True:\n",
    "                reg = l2(w, mini_batch_y)\n",
    "                current_cost = regularized_mean_square_error(mini_batch_y, y_hat, reg)\n",
    "            else: \n",
    "                current_cost = mean_square_error(y_hat, mini_batch_y)\n",
    "\n",
    "            if (past_cost < current_cost): # if past cost remains to be lower than current cost, increment counter\n",
    "                epochs_since_improved += 1\n",
    "\n",
    "                if epochs_since_improved >= 3:\n",
    "                    learning_rate /= 10 #learning rate decays if cost does not lower after 3 epochs \n",
    "                \n",
    "            else: # if current cost is lower than past cost set the counter to zero and update the past cost value\n",
    "                epoch_since_improved = 0 \n",
    "                past_cost = current_cost\n",
    "\n",
    "            w, b = gradient_descent(mini_batch_x, mini_batch_y, b, w, learning_rate, reg_bool)\n",
    "\n",
    "            print(\"MSE: \", current_cost)\n",
    "\n",
    "        history[\"epoch\"] += 1\n",
    "        history[\"weight\"] = w\n",
    "        history[\"bias\"] = b\n",
    "        history[\"cost\"] = current_cost\n",
    "        history[\"learning rate\"] = learning_rate\n",
    "\n",
    "    visualize_that_beh(history)\n",
    "    \n",
    "    return w, b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_loss(parameters, labels, learning_rate, num_of_epochs, size_of_batch, reg_bool = True):\n",
    "    np.random.seed(0)\n",
    "    w_shape = (parameters.shape[1], l.shape[1])\n",
    "    scale = 1/max(1., (parameters.shape[1] + l.shape[1])/2.)\n",
    "\n",
    "    size_of_dataset = len(labels)\n",
    "    l = np.expand_dims(a=labels, axis=-1)\n",
    "    w = () #set intial w value \n",
    "    b = np.zeros(l.shape[1]) #set initial bias value \n",
    "    reg = l2(w, labels) # set regularized term\n",
    "    w_new, b_new = stochastic_gradient_descent(parameters, labels, w, learning_rate, num_of_epochs, size_of_batch, size_of_dataset, reg_bool)\n",
    "    loss = train_valid_test(parameters, w, b, labels, reg, reg_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test(x, w, b, y, reg: float, reg_bool: bool = False):\n",
    "    y_hat = model(x, w, b)\n",
    "    if reg_bool:\n",
    "        train_or_validation = regularized_mean_square_error(y, y_hat, reg)\n",
    "        test = regularized_mean_square_error(y, y_hat, reg)\n",
    "    else:\n",
    "        train_or_validation = mean_square_error(y_hat, y)\n",
    "        test = mean_square_error(y_hat, y)\n",
    "\n",
    "    return train_or_validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #training set\n",
    "    x_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))\n",
    "    y_tr = np.load(\"age_regression_ytr.npy\")\n",
    "\n",
    "    #testing set\n",
    "    x_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "    y_te = np.load(\"age_regression_yte.npy\")\n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Arguments:\n",
    "#mse_bool is the boolean value that determines if mse will be performed without or with regularization, default is True\n",
    "#alpha is the value for regularization, if used, default is 0.0\n",
    "#ttv_val is the value associated with the type of split wanted. A Train/Test Split is 0, and a train/validation/split is 1, No Split Needed is 2. Default is 2\n",
    "#learning rate is the hyperparameter associated with the gradient descent\n",
    "def train_age_regressor(num_of_epochs: int, size_of_batch: int, ttv_val: int = 2, the_set: int = 0, learning_rate: float = 0.1):\n",
    "    # Load data\n",
    "    x_tr, y_tr, x_te, y_te = load_data()\n",
    "\n",
    "    if ttv_val == 0:\n",
    "        print(\"I made this before I realized train/test split is already given to you\")\n",
    "\n",
    "    elif ttv_val == 1:\n",
    "        x_val, y_val, x_te, y_te = random_test_validation_split(x_te, y_te, train_perc = 0.8)\n",
    "\n",
    "    loss = 1000000000\n",
    "    if the_set == 0: #training set\n",
    "        loss = find_lowest_loss(x_tr, y_tr, learning_rate, num_of_epochs, size_of_batch, False)\n",
    "    elif the_set == 1: #validation set\n",
    "        loss = find_lowest_loss(x_val, y_val, learning_rate, num_of_epochs, size_of_batch, False)\n",
    "    elif the_set == 2: #test set\n",
    "        loss = find_lowest_loss(x_te, y_te, learning_rate, num_of_epochs, size_of_batch, False)\n",
    "    else:\n",
    "        print(\"you did something wrong bud\")\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "    # Report fMSE cost on the training and testing data (separately)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  2049563.2087066027\n",
      "MSE:  7.975880252950522e+22\n",
      "MSE:  3.5303907205798003e+39\n",
      "MSE:  1.5635749000293804e+56\n",
      "MSE:  6.935080030574707e+70\n",
      "MSE:  3.078489195950324e+83\n",
      "MSE:  1.3648993255958013e+94\n",
      "MSE:  6.044563967050886e+102\n",
      "MSE:  2.681025415789095e+109\n",
      "MSE:  1.1809529851555533e+114\n",
      "MSE:  4.7665526765554815e+116\n",
      "MSE:  5.8476105882848335e+116\n",
      "MSE:  3.649667937322716e+116\n",
      "MSE:  3.501693263543795e+116\n",
      "MSE:  3.490063088951129e+116\n",
      "MSE:  3.484395772899047e+116\n",
      "MSE:  3.486112022233724e+116\n",
      "MSE:  3.482652353435932e+116\n",
      "MSE:  3.487906483932186e+116\n",
      "MSE:  3.4880584064573615e+116\n",
      "MSE:  3.4780201867450385e+116\n",
      "MSE:  3.487442033324012e+116\n",
      "MSE:  3.4837788904161785e+116\n",
      "MSE:  3.490396176154795e+116\n",
      "MSE:  3.4827663788663536e+116\n",
      "MSE:  3.4874301595804625e+116\n",
      "MSE:  3.489290522397093e+116\n",
      "MSE:  3.49001898905978e+116\n",
      "MSE:  3.490484808630409e+116\n",
      "MSE:  3.489810302212906e+116\n",
      "MSE:  3.4757468504680626e+116\n",
      "MSE:  3.484780884796922e+116\n",
      "MSE:  3.487844351446385e+116\n",
      "MSE:  3.482925331607204e+116\n",
      "MSE:  3.483789286022871e+116\n",
      "MSE:  3.478279006458616e+116\n",
      "MSE:  3.484307995493665e+116\n",
      "MSE:  3.491777039100074e+116\n",
      "MSE:  3.484844679645664e+116\n",
      "MSE:  3.480200256225678e+116\n",
      "MSE:  3.494905430017823e+116\n",
      "MSE:  3.4785161161366375e+116\n",
      "MSE:  3.490993090959465e+116\n",
      "MSE:  3.483878425941183e+116\n",
      "MSE:  3.489583980002629e+116\n",
      "MSE:  3.483459074948049e+116\n",
      "MSE:  3.4769065930196686e+116\n",
      "MSE:  3.4802737015735014e+116\n",
      "MSE:  3.483606558057841e+116\n",
      "MSE:  3.468130134373564e+116\n",
      "MSE:  3.4882253751437526e+116\n",
      "MSE:  3.480185727465465e+116\n",
      "MSE:  3.4898370280470784e+116\n",
      "MSE:  3.480158548126097e+116\n",
      "MSE:  3.490936119090192e+116\n",
      "MSE:  3.4814780350098286e+116\n",
      "MSE:  3.4847628812953526e+116\n",
      "MSE:  3.480429397531018e+116\n",
      "MSE:  3.477106149687633e+116\n",
      "MSE:  3.488994580078218e+116\n",
      "MSE:  3.484840579329378e+116\n",
      "MSE:  3.478116472409683e+116\n",
      "MSE:  3.4804416457863284e+116\n",
      "MSE:  3.482063521425813e+116\n",
      "MSE:  3.492269898944259e+116\n",
      "MSE:  3.488574000815008e+116\n",
      "MSE:  3.4930478866879286e+116\n",
      "MSE:  3.485547929911297e+116\n",
      "MSE:  3.488852573263773e+116\n",
      "MSE:  3.4830691364892545e+116\n",
      "MSE:  3.4840210529681156e+116\n",
      "MSE:  3.491358321329942e+116\n",
      "MSE:  3.493371759229247e+116\n",
      "MSE:  3.4809032620772084e+116\n",
      "MSE:  3.490359745320216e+116\n",
      "MSE:  3.4853410189831914e+116\n",
      "MSE:  3.489867461003548e+116\n",
      "MSE:  3.491803805786494e+116\n",
      "MSE:  3.484582283573468e+116\n",
      "MSE:  3.488833410044873e+116\n",
      "MSE:  3.48635242293897e+116\n",
      "MSE:  3.4803992462368726e+116\n",
      "MSE:  3.479168124537074e+116\n",
      "MSE:  3.4836584748707934e+116\n",
      "MSE:  3.483146361806966e+116\n",
      "MSE:  3.479097676724213e+116\n",
      "MSE:  3.488372889331131e+116\n",
      "MSE:  3.479467529175089e+116\n",
      "MSE:  3.490457621941829e+116\n",
      "MSE:  3.495327253775724e+116\n",
      "MSE:  3.492169916694391e+116\n",
      "MSE:  3.483975569043157e+116\n",
      "MSE:  3.481943199464876e+116\n",
      "MSE:  3.4816032576851306e+116\n",
      "MSE:  3.4852555043e+116\n",
      "MSE:  3.481511701995906e+116\n",
      "MSE:  3.480824944740162e+116\n",
      "MSE:  3.4900623102450504e+116\n",
      "MSE:  3.490780298093832e+116\n",
      "MSE:  3.500964625273397e+116\n",
      "MSE:  3.478031825010407e+116\n",
      "MSE:  3.482494335301215e+116\n",
      "MSE:  3.4859979717095084e+116\n",
      "MSE:  3.476638528585234e+116\n",
      "MSE:  3.4760736992692394e+116\n",
      "MSE:  3.4900670706583735e+116\n",
      "MSE:  3.4915957824605205e+116\n",
      "MSE:  3.487603008026073e+116\n",
      "MSE:  3.489813058803211e+116\n",
      "MSE:  3.486551512416741e+116\n",
      "MSE:  3.4804248289804624e+116\n",
      "MSE:  3.486266098274202e+116\n",
      "MSE:  3.483838617810682e+116\n",
      "MSE:  3.483891121241227e+116\n",
      "MSE:  3.4791417536959385e+116\n",
      "MSE:  3.487436379051523e+116\n",
      "MSE:  3.4874312278450566e+116\n",
      "MSE:  3.486896598404234e+116\n",
      "MSE:  3.486626253024892e+116\n",
      "MSE:  3.482090334280627e+116\n",
      "MSE:  3.482857303912052e+116\n",
      "MSE:  3.48947917924024e+116\n",
      "MSE:  3.487615639091484e+116\n",
      "MSE:  3.4888359842553644e+116\n",
      "MSE:  3.486194946287696e+116\n",
      "MSE:  3.487372224657776e+116\n",
      "MSE:  3.49070387052275e+116\n",
      "MSE:  3.474379249219102e+116\n",
      "MSE:  3.483896993028893e+116\n",
      "MSE:  3.473891191056487e+116\n",
      "MSE:  3.488969925488873e+116\n",
      "MSE:  3.4911416308401614e+116\n",
      "MSE:  3.483728105403559e+116\n",
      "MSE:  3.4910835096000565e+116\n",
      "MSE:  3.486890207712028e+116\n",
      "MSE:  3.481829880237434e+116\n",
      "MSE:  3.483354375118449e+116\n",
      "MSE:  3.493006843460064e+116\n",
      "MSE:  3.487851651088127e+116\n",
      "MSE:  3.486674487368575e+116\n",
      "MSE:  3.481130485717002e+116\n",
      "MSE:  3.4806068423715605e+116\n",
      "MSE:  3.4776259792346686e+116\n",
      "MSE:  3.483454221859911e+116\n",
      "MSE:  3.485611289171319e+116\n",
      "MSE:  3.490078265979617e+116\n",
      "MSE:  3.483835381662793e+116\n",
      "MSE:  3.4825569912282755e+116\n",
      "MSE:  3.489276266502082e+116\n",
      "MSE:  3.487214300062036e+116\n",
      "MSE:  3.4908468336917145e+116\n",
      "MSE:  3.482260536941336e+116\n",
      "MSE:  3.481792316819394e+116\n",
      "MSE:  3.475033987243087e+116\n",
      "MSE:  3.4890994940953235e+116\n",
      "MSE:  3.4881748561279976e+116\n",
      "MSE:  3.472949152984266e+116\n",
      "MSE:  3.476101171344102e+116\n",
      "MSE:  3.49461980709937e+116\n",
      "MSE:  3.4871501252506265e+116\n",
      "MSE:  3.4808199911819664e+116\n",
      "MSE:  3.481436430131766e+116\n",
      "MSE:  3.484868730939097e+116\n",
      "MSE:  3.482925512316887e+116\n",
      "MSE:  3.490873689791281e+116\n",
      "MSE:  3.482103964529471e+116\n",
      "MSE:  3.4878391450376494e+116\n",
      "MSE:  3.491542896908745e+116\n",
      "MSE:  3.480109453760268e+116\n",
      "MSE:  3.4816205998875024e+116\n",
      "MSE:  3.484897305925346e+116\n",
      "MSE:  3.479798971772208e+116\n",
      "MSE:  3.486381188386128e+116\n",
      "MSE:  3.492213769313846e+116\n",
      "MSE:  3.4841651235299706e+116\n",
      "MSE:  3.487157152865776e+116\n",
      "MSE:  3.483393621900075e+116\n",
      "MSE:  3.4832475869247877e+116\n",
      "MSE:  3.4810488407751695e+116\n",
      "MSE:  3.485430096644158e+116\n",
      "MSE:  3.485677967074139e+116\n",
      "MSE:  3.483526576379754e+116\n",
      "MSE:  3.4850859231605184e+116\n",
      "MSE:  3.493340532786706e+116\n",
      "MSE:  3.482412505897526e+116\n",
      "MSE:  3.494399594931246e+116\n",
      "MSE:  3.4842040228736224e+116\n",
      "MSE:  3.482485788253993e+116\n",
      "MSE:  3.484182901367074e+116\n",
      "MSE:  3.484969190635082e+116\n",
      "MSE:  3.492934380826272e+116\n",
      "MSE:  3.487668446011908e+116\n",
      "MSE:  3.492696000494731e+116\n",
      "MSE:  3.480855007921152e+116\n",
      "MSE:  3.484372198438264e+116\n",
      "MSE:  3.489700333068158e+116\n",
      "MSE:  3.484170663538592e+116\n",
      "MSE:  3.4741707035728695e+116\n",
      "MSE:  3.4808361940852996e+116\n",
      "MSE:  3.486389952550268e+116\n",
      "MSE:  3.488940633655743e+116\n",
      "MSE:  3.483321576638297e+116\n",
      "MSE:  3.486242601889677e+116\n",
      "MSE:  3.491992443299159e+116\n",
      "MSE:  3.485176476601903e+116\n",
      "MSE:  3.480851568678248e+116\n",
      "MSE:  3.4834400240509145e+116\n",
      "MSE:  3.483127572176277e+116\n",
      "MSE:  3.493702698546498e+116\n",
      "MSE:  3.4899960111574444e+116\n",
      "MSE:  3.482022114966792e+116\n",
      "MSE:  3.477414887681668e+116\n",
      "MSE:  3.486871508009253e+116\n",
      "MSE:  3.484043040117581e+116\n",
      "MSE:  3.4937493342655875e+116\n",
      "MSE:  3.4842764423578205e+116\n",
      "MSE:  3.4761697680470404e+116\n",
      "MSE:  3.4870744241086605e+116\n",
      "MSE:  3.483788004655568e+116\n",
      "MSE:  3.485070202890069e+116\n",
      "MSE:  3.4796458270431715e+116\n",
      "MSE:  3.486109533746037e+116\n",
      "MSE:  3.488628611304557e+116\n",
      "MSE:  3.489258053920668e+116\n",
      "MSE:  3.491179634301476e+116\n",
      "MSE:  3.4879120493047715e+116\n",
      "MSE:  3.483625473947805e+116\n",
      "MSE:  3.49215152282122e+116\n",
      "MSE:  3.485081799264294e+116\n",
      "MSE:  3.476614419197608e+116\n",
      "MSE:  3.489753104569244e+116\n",
      "MSE:  3.491687844076628e+116\n",
      "MSE:  3.476853979736516e+116\n",
      "MSE:  3.487258971828922e+116\n",
      "MSE:  3.47771672104434e+116\n",
      "MSE:  3.477973543827678e+116\n",
      "MSE:  3.4964741757474355e+116\n",
      "MSE:  3.486287675198056e+116\n",
      "MSE:  3.484677977395474e+116\n",
      "MSE:  3.484611393046468e+116\n",
      "MSE:  3.482598997018905e+116\n",
      "MSE:  3.4864722988581036e+116\n",
      "MSE:  3.483205729959865e+116\n",
      "MSE:  3.490406123297969e+116\n",
      "MSE:  3.4793086496920404e+116\n",
      "MSE:  3.483986729398015e+116\n",
      "MSE:  3.482492928767603e+116\n",
      "MSE:  3.48296971330808e+116\n",
      "MSE:  3.483931696633566e+116\n",
      "MSE:  3.4925711327337666e+116\n",
      "MSE:  3.494121844623968e+116\n",
      "MSE:  3.483742904397661e+116\n",
      "MSE:  3.480591779283135e+116\n",
      "MSE:  3.4822305094447236e+116\n",
      "MSE:  3.481812607705031e+116\n",
      "MSE:  3.48437722850573e+116\n",
      "MSE:  3.4830149346421436e+116\n",
      "MSE:  3.4860832132929574e+116\n",
      "MSE:  3.4894929393821075e+116\n",
      "MSE:  3.489799908897736e+116\n",
      "MSE:  3.475134520764715e+116\n",
      "MSE:  3.477312277107623e+116\n",
      "MSE:  3.4796067608383446e+116\n",
      "MSE:  3.480074598063587e+116\n",
      "MSE:  3.47568525114488e+116\n",
      "MSE:  3.488269971740978e+116\n",
      "MSE:  3.492368056587703e+116\n",
      "MSE:  3.4798189356753516e+116\n",
      "MSE:  3.479549943759472e+116\n",
      "MSE:  3.492309211631406e+116\n",
      "MSE:  3.490670011542743e+116\n",
      "MSE:  3.478873531073305e+116\n",
      "MSE:  3.485038616607521e+116\n",
      "MSE:  3.4913095851159725e+116\n",
      "MSE:  3.4884041627311666e+116\n",
      "MSE:  3.480203199070053e+116\n",
      "MSE:  3.4800299667867704e+116\n",
      "MSE:  3.479586533634927e+116\n",
      "MSE:  3.4922951051473005e+116\n",
      "MSE:  3.4873264716280845e+116\n",
      "MSE:  3.4772088579249425e+116\n",
      "MSE:  3.480762127047314e+116\n",
      "MSE:  3.482895167973532e+116\n",
      "MSE:  3.4846685095367724e+116\n",
      "MSE:  3.4867168931653874e+116\n",
      "MSE:  3.480866660669677e+116\n",
      "MSE:  3.488724346074439e+116\n",
      "MSE:  3.4871810617159174e+116\n",
      "MSE:  3.4841826958772216e+116\n",
      "MSE:  3.48325729724659e+116\n",
      "MSE:  3.4879773533708934e+116\n",
      "MSE:  3.488973451185305e+116\n",
      "MSE:  3.484891269099407e+116\n",
      "MSE:  3.483034638686186e+116\n",
      "MSE:  3.475968626266717e+116\n",
      "MSE:  3.488855302125261e+116\n",
      "MSE:  3.49017273799655e+116\n",
      "MSE:  3.4809683583750884e+116\n",
      "MSE:  3.4790990673376104e+116\n",
      "MSE:  3.488199625564118e+116\n",
      "MSE:  3.477824615211809e+116\n",
      "MSE:  3.487780282142134e+116\n",
      "MSE:  3.48073801879467e+116\n",
      "MSE:  3.484085575304946e+116\n",
      "MSE:  3.4863030570605676e+116\n",
      "MSE:  3.4868226494860484e+116\n",
      "MSE:  3.482989086776448e+116\n",
      "MSE:  3.486577302322062e+116\n",
      "MSE:  3.485196350229331e+116\n",
      "MSE:  3.476340395526427e+116\n",
      "MSE:  3.4780806329736634e+116\n",
      "MSE:  3.4826394621598995e+116\n",
      "MSE:  3.481621398604276e+116\n",
      "MSE:  3.484490433182078e+116\n",
      "MSE:  3.490842895403951e+116\n",
      "MSE:  3.4820464562826204e+116\n",
      "MSE:  3.4872690147914144e+116\n",
      "MSE:  3.497050490253749e+116\n",
      "MSE:  3.4833748761717376e+116\n",
      "MSE:  3.4859871475493065e+116\n",
      "MSE:  3.483320808930573e+116\n",
      "MSE:  3.490388866708135e+116\n",
      "MSE:  3.476473850597511e+116\n",
      "MSE:  3.482578459592606e+116\n",
      "MSE:  3.486393447940043e+116\n",
      "MSE:  3.477406447358682e+116\n",
      "MSE:  3.4838092432109854e+116\n",
      "MSE:  3.484533820927082e+116\n",
      "MSE:  3.4798358448507915e+116\n",
      "MSE:  3.480299785484134e+116\n",
      "MSE:  3.47650390520325e+116\n",
      "MSE:  3.487939754739358e+116\n",
      "MSE:  3.481005961765562e+116\n",
      "MSE:  3.489610586829431e+116\n",
      "MSE:  3.48282639580013e+116\n",
      "MSE:  3.481109540946361e+116\n",
      "MSE:  3.487228899005926e+116\n",
      "MSE:  3.486416731264107e+116\n",
      "MSE:  3.4930334359202974e+116\n",
      "MSE:  3.480011669536466e+116\n",
      "MSE:  3.482288971912407e+116\n",
      "MSE:  3.482470274056898e+116\n",
      "MSE:  3.490648780326635e+116\n",
      "MSE:  3.482068002824281e+116\n",
      "MSE:  3.479080774662953e+116\n",
      "MSE:  3.4871810010496776e+116\n",
      "MSE:  3.48230020360787e+116\n",
      "MSE:  3.485769839346606e+116\n",
      "MSE:  3.492658385432513e+116\n",
      "MSE:  3.476147127993969e+116\n",
      "MSE:  3.489586962232429e+116\n",
      "MSE:  3.485588018291851e+116\n",
      "MSE:  3.475686094955603e+116\n",
      "MSE:  3.490795667097802e+116\n",
      "MSE:  3.479549668685589e+116\n",
      "MSE:  3.4780574473628056e+116\n",
      "MSE:  3.480531711341673e+116\n",
      "MSE:  3.475209114288357e+116\n",
      "MSE:  3.486050636095496e+116\n",
      "MSE:  3.490041333057288e+116\n",
      "MSE:  3.4910957528078395e+116\n",
      "MSE:  3.4870432141662215e+116\n",
      "MSE:  3.485226537819844e+116\n",
      "MSE:  3.495404793504028e+116\n",
      "MSE:  3.480180812460595e+116\n",
      "MSE:  3.484711668107117e+116\n",
      "MSE:  3.483219729430987e+116\n",
      "MSE:  3.482869184107913e+116\n",
      "MSE:  3.4890911637243894e+116\n",
      "MSE:  3.483529071583789e+116\n",
      "MSE:  3.487889937362021e+116\n",
      "MSE:  3.481881187051483e+116\n",
      "MSE:  3.48353620613282e+116\n",
      "MSE:  3.491954235332957e+116\n",
      "MSE:  3.498171835041135e+116\n",
      "MSE:  3.4833518453769764e+116\n",
      "MSE:  3.477313699273108e+116\n",
      "MSE:  3.477382172720558e+116\n",
      "MSE:  3.47605849248488e+116\n",
      "MSE:  3.4768722442615974e+116\n",
      "MSE:  3.49007861546078e+116\n",
      "MSE:  3.4794556785614856e+116\n",
      "MSE:  3.482891958284629e+116\n",
      "MSE:  3.4791165005379924e+116\n",
      "MSE:  3.480824605497873e+116\n",
      "MSE:  3.4761809650787396e+116\n",
      "MSE:  3.4826291113144776e+116\n",
      "MSE:  3.490268895541976e+116\n",
      "MSE:  3.4842840349724406e+116\n",
      "MSE:  3.4914241880111854e+116\n",
      "MSE:  3.4776015270227555e+116\n",
      "MSE:  3.490045513062233e+116\n",
      "MSE:  3.490592942383323e+116\n",
      "MSE:  3.480249612559698e+116\n",
      "MSE:  3.476429766413057e+116\n",
      "MSE:  3.4827510487268864e+116\n",
      "MSE:  3.4836560536683805e+116\n",
      "MSE:  3.4854590210173086e+116\n",
      "MSE:  3.484591962528017e+116\n",
      "MSE:  3.4808129546056026e+116\n",
      "MSE:  3.493478779330952e+116\n",
      "MSE:  3.47784528570254e+116\n",
      "MSE:  3.482845248431304e+116\n",
      "MSE:  3.479417622755039e+116\n",
      "MSE:  3.488590770820804e+116\n",
      "MSE:  3.482382833708693e+116\n",
      "MSE:  3.480933169097313e+116\n",
      "MSE:  3.486494668210667e+116\n",
      "MSE:  3.487068753060682e+116\n",
      "MSE:  3.485314462032911e+116\n",
      "MSE:  3.488570411344029e+116\n",
      "MSE:  3.4881977251113884e+116\n",
      "MSE:  3.48478044638245e+116\n",
      "MSE:  3.487518982793331e+116\n",
      "MSE:  3.4827946463176406e+116\n",
      "MSE:  3.480006203938807e+116\n",
      "MSE:  3.4779030579071385e+116\n",
      "MSE:  3.4873548759204464e+116\n",
      "MSE:  3.477775589832708e+116\n",
      "MSE:  3.482310745148013e+116\n",
      "MSE:  3.4791038293693716e+116\n",
      "MSE:  3.4846608400142166e+116\n",
      "MSE:  3.4780863826090995e+116\n",
      "MSE:  3.476347344426755e+116\n",
      "MSE:  3.4860325532400434e+116\n",
      "MSE:  3.4794770845835694e+116\n",
      "MSE:  3.491206507234083e+116\n",
      "MSE:  3.481432585333162e+116\n",
      "MSE:  3.486088219326187e+116\n",
      "MSE:  3.484868182216748e+116\n",
      "MSE:  3.487973912962178e+116\n",
      "MSE:  3.486349321218667e+116\n",
      "MSE:  3.4864461286982686e+116\n",
      "MSE:  3.475547934914107e+116\n",
      "MSE:  3.4837947939344754e+116\n",
      "MSE:  3.4811562545174435e+116\n",
      "MSE:  3.479771850315175e+116\n",
      "MSE:  3.474927437311257e+116\n",
      "MSE:  3.495143694270971e+116\n",
      "MSE:  3.485122250784559e+116\n",
      "MSE:  3.4837270535754176e+116\n",
      "MSE:  3.487096021394357e+116\n",
      "MSE:  3.489914649370577e+116\n",
      "MSE:  3.486768927523317e+116\n",
      "MSE:  3.4786785618951256e+116\n",
      "MSE:  3.490735745939998e+116\n",
      "MSE:  3.488309337460896e+116\n",
      "MSE:  3.4816471149379905e+116\n",
      "MSE:  3.499069065720087e+116\n",
      "MSE:  3.483312447298435e+116\n",
      "MSE:  3.478236482878658e+116\n",
      "MSE:  3.489240218814411e+116\n",
      "MSE:  3.4900415851505915e+116\n",
      "MSE:  3.488896718480863e+116\n",
      "MSE:  3.487337126296805e+116\n",
      "MSE:  3.4902970251014316e+116\n",
      "MSE:  3.480446728090955e+116\n",
      "MSE:  3.4880953002824784e+116\n",
      "MSE:  3.476862924769614e+116\n",
      "MSE:  3.4846988144125995e+116\n",
      "MSE:  3.485084447828956e+116\n",
      "MSE:  3.4849697675300625e+116\n",
      "MSE:  3.4801075007279746e+116\n",
      "MSE:  3.491520540893588e+116\n",
      "MSE:  3.491329809388816e+116\n",
      "MSE:  3.481780149529741e+116\n",
      "MSE:  3.493054183662287e+116\n",
      "MSE:  3.4933812804804775e+116\n",
      "MSE:  3.4864228299925276e+116\n",
      "MSE:  3.488495248289454e+116\n",
      "MSE:  3.470396605381875e+116\n",
      "MSE:  3.4849359631882436e+116\n",
      "MSE:  3.4808291327570414e+116\n",
      "MSE:  3.482909423336961e+116\n",
      "MSE:  3.482391366500051e+116\n",
      "MSE:  3.482214741082969e+116\n",
      "MSE:  3.485577924737766e+116\n",
      "MSE:  3.4856237112673916e+116\n",
      "MSE:  3.471304521983139e+116\n",
      "MSE:  3.476610007610143e+116\n",
      "MSE:  3.481005891968846e+116\n",
      "MSE:  3.498262732471633e+116\n",
      "MSE:  3.48457588925322e+116\n",
      "MSE:  3.489069784911774e+116\n",
      "MSE:  3.486956836420912e+116\n",
      "MSE:  3.485377570394333e+116\n",
      "MSE:  3.490918535965982e+116\n",
      "MSE:  3.493543250269242e+116\n",
      "MSE:  3.483471658793804e+116\n",
      "MSE:  3.487328366305039e+116\n",
      "MSE:  3.482317485536968e+116\n",
      "MSE:  3.486916423906767e+116\n",
      "MSE:  3.483808672564075e+116\n",
      "MSE:  3.485633760635774e+116\n",
      "MSE:  3.4919551559507865e+116\n",
      "MSE:  3.491974076315166e+116\n",
      "MSE:  3.48189732028032e+116\n",
      "MSE:  3.472199811790811e+116\n",
      "MSE:  3.476754878874774e+116\n",
      "MSE:  3.485261924720586e+116\n",
      "MSE:  3.484849107907395e+116\n",
      "MSE:  3.486132651412146e+116\n",
      "MSE:  3.4770230410638826e+116\n",
      "MSE:  3.479536990878366e+116\n",
      "MSE:  3.492086477032639e+116\n",
      "MSE:  3.4784112831909785e+116\n",
      "MSE:  3.486194197003534e+116\n",
      "MSE:  3.490797449331686e+116\n",
      "MSE:  3.491055101697952e+116\n",
      "MSE:  3.487713396137784e+116\n",
      "MSE:  3.482080349095874e+116\n",
      "MSE:  3.4783754020683205e+116\n",
      "MSE:  3.4785404009426185e+116\n",
      "MSE:  3.479248890387044e+116\n",
      "MSE:  3.4888570351303486e+116\n",
      "MSE:  3.486015880585856e+116\n",
      "MSE:  3.4842695251726334e+116\n",
      "MSE:  3.481439652169982e+116\n",
      "MSE:  3.480791021782559e+116\n",
      "MSE:  3.4816184709749546e+116\n",
      "MSE:  3.487426657419767e+116\n",
      "MSE:  3.490776330166561e+116\n",
      "MSE:  3.479537806677982e+116\n",
      "MSE:  3.487157663724742e+116\n",
      "MSE:  3.4812451890763876e+116\n",
      "MSE:  3.48088928431887e+116\n",
      "MSE:  3.4930113372080206e+116\n",
      "MSE:  3.4910023342695746e+116\n",
      "MSE:  3.483669164716573e+116\n",
      "MSE:  3.484688077039553e+116\n",
      "MSE:  3.4938044276574e+116\n",
      "MSE:  3.482240281552392e+116\n",
      "MSE:  3.49259975094328e+116\n",
      "MSE:  3.48225395438322e+116\n",
      "MSE:  3.489118317757758e+116\n",
      "MSE:  3.493493771432546e+116\n",
      "MSE:  3.484118178667362e+116\n",
      "MSE:  3.472360579957955e+116\n",
      "MSE:  3.487820334473007e+116\n",
      "MSE:  3.48006536847173e+116\n",
      "MSE:  3.480773159908462e+116\n",
      "MSE:  3.487002370650278e+116\n",
      "MSE:  3.489421779914174e+116\n",
      "MSE:  3.4909085791748426e+116\n",
      "MSE:  3.485887600713378e+116\n",
      "MSE:  3.48745481698502e+116\n",
      "MSE:  3.482984050768806e+116\n",
      "MSE:  3.487743298476025e+116\n",
      "MSE:  3.483449714688715e+116\n",
      "MSE:  3.4862449598599265e+116\n",
      "MSE:  3.487923822194399e+116\n",
      "MSE:  3.4855145483284094e+116\n",
      "MSE:  3.488042727374012e+116\n",
      "MSE:  3.486055386042026e+116\n",
      "MSE:  3.479625319151283e+116\n",
      "MSE:  3.4864198653770875e+116\n",
      "MSE:  3.484429873540902e+116\n",
      "MSE:  3.4828739554812834e+116\n",
      "MSE:  3.4847989650824035e+116\n",
      "MSE:  3.48222511798021e+116\n",
      "MSE:  3.470264075944937e+116\n",
      "MSE:  3.4869247798941484e+116\n",
      "MSE:  3.484396781654846e+116\n",
      "MSE:  3.4851784402941744e+116\n",
      "MSE:  3.481222352107315e+116\n",
      "MSE:  3.484984548934296e+116\n",
      "MSE:  3.481910556807116e+116\n",
      "MSE:  3.475181419659995e+116\n",
      "MSE:  3.4978140671632506e+116\n",
      "MSE:  3.480694878898031e+116\n",
      "MSE:  3.4834170745337716e+116\n",
      "MSE:  3.4824741943998035e+116\n",
      "MSE:  3.4873555573932636e+116\n",
      "MSE:  3.485756649135367e+116\n",
      "MSE:  3.487001719154321e+116\n",
      "MSE:  3.4875621383537874e+116\n",
      "MSE:  3.493295508132291e+116\n",
      "MSE:  3.483517266093298e+116\n",
      "MSE:  3.484390161708924e+116\n",
      "MSE:  3.485777608311517e+116\n",
      "MSE:  3.491809850898324e+116\n",
      "MSE:  3.4996196017445405e+116\n",
      "MSE:  3.482196358419593e+116\n",
      "MSE:  3.480307707981884e+116\n",
      "MSE:  3.489424155472429e+116\n",
      "MSE:  3.48930788786218e+116\n",
      "MSE:  3.4802786832263926e+116\n",
      "MSE:  3.483976923709265e+116\n",
      "MSE:  3.4861136351000055e+116\n",
      "MSE:  3.480554948967045e+116\n",
      "MSE:  3.479497940026021e+116\n",
      "MSE:  3.480939730598151e+116\n",
      "MSE:  3.485607365769078e+116\n",
      "MSE:  3.487306639201719e+116\n",
      "MSE:  3.481207902969495e+116\n",
      "MSE:  3.481483894233374e+116\n",
      "MSE:  3.5006212079261356e+116\n",
      "MSE:  3.4949250793607e+116\n",
      "MSE:  3.4961003048103574e+116\n",
      "MSE:  3.4767782070496955e+116\n",
      "MSE:  3.483741319131387e+116\n",
      "MSE:  3.4832556798125365e+116\n",
      "MSE:  3.484942034562588e+116\n",
      "MSE:  3.493499833318941e+116\n",
      "MSE:  3.4867609437377524e+116\n",
      "MSE:  3.480202346750483e+116\n",
      "MSE:  3.478511095446055e+116\n",
      "MSE:  3.479208514008533e+116\n",
      "MSE:  3.4822725817032716e+116\n",
      "MSE:  3.483116633819946e+116\n",
      "MSE:  3.489810383641353e+116\n",
      "MSE:  3.484855734911937e+116\n",
      "MSE:  3.485951261722468e+116\n",
      "MSE:  3.484424114699815e+116\n",
      "MSE:  3.4870124064001105e+116\n",
      "MSE:  3.482784910809937e+116\n",
      "MSE:  3.482702503523747e+116\n",
      "MSE:  3.4857845943513774e+116\n",
      "MSE:  3.480786317114101e+116\n",
      "MSE:  3.4851022083947364e+116\n",
      "MSE:  3.4759692634331796e+116\n",
      "MSE:  3.487164366827985e+116\n",
      "MSE:  3.494182226293435e+116\n",
      "MSE:  3.4782497404174534e+116\n",
      "MSE:  3.486888122706647e+116\n",
      "MSE:  3.477003081112672e+116\n",
      "MSE:  3.481176332003835e+116\n",
      "MSE:  3.484605609421561e+116\n",
      "MSE:  3.4867828075429984e+116\n",
      "MSE:  3.473612655942984e+116\n",
      "MSE:  3.493648744212598e+116\n",
      "MSE:  3.485920067312972e+116\n",
      "MSE:  3.4935407395264776e+116\n",
      "MSE:  3.488174935407738e+116\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8f234d0f43ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_age_regressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize_of_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttv_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-da6656f8c766>\u001b[0m in \u001b[0;36mtrain_age_regressor\u001b[1;34m(num_of_epochs, size_of_batch, ttv_val, the_set, learning_rate)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000000000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mthe_set\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_lowest_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_of_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mthe_set\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_lowest_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_of_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-de2edc74902e>\u001b[0m in \u001b[0;36mfind_lowest_loss\u001b[1;34m(parameters, labels, learning_rate, num_of_epochs, size_of_batch, reg_bool)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#set initial bias value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# set regularized term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mw_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_of_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_of_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_bool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_valid_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_bool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-8f77bf892f9c>\u001b[0m in \u001b[0;36mstochastic_gradient_descent\u001b[1;34m(x, y, w, learning_rate, num_of_epochs, size_of_batch, size_of_dataset, reg_bool)\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mpast_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_bool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSE: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-956ed22a6e6b>\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(x, y, b, w, epsilon, reg_bool)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#taking derivative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mderivative_of_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mderivative_of_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_age_regressor(num_of_epochs=300,size_of_batch=100, ttv_val=1, the_set=0, learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}