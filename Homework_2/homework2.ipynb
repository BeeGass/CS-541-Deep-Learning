{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "hyperparameters = {\n",
    "    number_of_epochs: 20, \n",
    "    batch_size: 100, \n",
    "    learning_rate: .01, \n",
    "    regularized_rate: .01\n",
    "    }\n",
    "\n",
    "preferences = {\n",
    "    \"epochs\": [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"batch\": [50, 75, 100, 125, 150, 200, 250, 300],\n",
    "    \"learning_rate\": [.1, .01, .001, .0001, .5, .05, .005, .0005],\n",
    "    \"regularized_rate\": [.1, .01, .001, .0001, .5, .05, .005, .0005]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression (x, y):\n",
    "    X_t = x.T\n",
    "    X_y = X_t.dot(y) # x is a column vector so it needs to be transformed\n",
    "    X_XT = X_t.dot(x)\n",
    "    w = np.linalg.solve(X_XT, X_y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    y_hat = x.dot(w) + b\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(y_hat, y):\n",
    "    n = len(y)\n",
    "    mse = (1/(2 * n)) * np.sum(np.square(y_hat-y))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2(w, alpha):\n",
    "    wt_w = (w).dot(w.T) # (w^T)w\n",
    "    reg = (1/(2 * n) * alpha) * wt_w # (alpha/2n) * ((w^T)w) \n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_mean_square_error(y, y_hat, reg):\n",
    "    n = len(y)\n",
    "    mse = mean_square_error(y_hat, y)\n",
    "    return mse + reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a training group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.8\n",
    "def random_train_test_split(dataset, train_perc = 0.8):\n",
    "    perc_of_dataset = dataset.shape[1] * train_perc\n",
    "    numpy.random.shuffle(dataset)\n",
    "\n",
    "    train = dataset_arrx[:perc_of_dataset,:] \n",
    "    test = dataset_arr[perc_of_dataset:,:]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation/Test Split performed by randomly taking inputs and their associated labels and assigning them to either a validation group or a test group\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                   |                |\n",
    "#                  Training set                     |   Testing Set  |\n",
    "#                                                   |                |\n",
    "#---------------------------------------------------#----------------#\n",
    "#                                                             V\n",
    "#                                                |------------#---------#\n",
    "#                                                |            |         |\n",
    "#                                                | Validation | Testing |\n",
    "#                                                |     Set    |   Set   |\n",
    "#                                                |------------#---------#\n",
    "#train_perc should be a value between 0 and 1, eg. train_perc 0.5 for a 50/50 split of the test set \n",
    "def random_test_validation_split(parameter, labels, train_perc = 0.8):\n",
    "    dataset_row_size, dataset_col_size = dataset.shape\n",
    "    validation_set_size = dataset_row_size * train_perc\n",
    "    test_set_size = dataset_row_size - validation_set_size\n",
    "\n",
    "    indices = numpy.random.permutation(parameter.shape[0])\n",
    "    validation_p_index = indices[:validation_set_size]\n",
    "    test_p_index = indices[test_set_size:]\n",
    "\n",
    "    validation_l, test_l = labels[validation_p_idx,:], labels[test_p_idx,:]\n",
    "    validation_p, test_p = parameter[validation_p_idx,:], parameter[test_p_idx,:]\n",
    "\n",
    "    return validation_p, validation_l, test_p, test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is the input \n",
    "#y is the real result\n",
    "#w is the weight \n",
    "#epsilon is the learning rate\n",
    "#alpha is the regularized term\n",
    "#reg_bool if you wish to use the regularized term\n",
    "def gradient_descent(x, y, b, w, epsilon, alpha, reg_bool):\n",
    "    X_t = x.t\n",
    "    n = len(y)\n",
    "    y_hat = model(x, w, b)\n",
    "\n",
    "    #taking derivative \n",
    "    derivative_of_w = (1/n) * np.sum(x.dot(y_hat - y))\n",
    "    derivative_of_b = (1/n) * np.sum(y_hat - y)\n",
    "\n",
    "    # if regularized term is wanted then add that to the derivative with respect to w\n",
    "    if reg_bool == true:\n",
    "        derivative_of_w += ((alpha/n) * w)\n",
    "\n",
    "    #this is gradient descent\n",
    "    w_new = w - (epsilon * derivative_of_w)\n",
    "    b_new = b - (epsilon * derivative_of_b)\n",
    "\n",
    "    return w_new, b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, w, learning_rate: float, num_of_epochs: int, size_of_batch: int, alpha: float, epsilon: float, n: int):\n",
    "\n",
    "    past_cost = 10000000\n",
    "    epochs_since_improved = 0\n",
    "    current_cost = 0 \n",
    "    for e in range(num_of_epochs - 1):\n",
    "        #TODO shuffle set\n",
    "        for b in range((math.ceil(n/size_of_batch)) - 1):\n",
    "            #TODO draw from batch iterator\n",
    "            y_hat = model(x, w, b)\n",
    "            current cost = regularized_mean_square_error(y, y_hat, reg)\n",
    "\n",
    "            if (past_cost < current_cost):\n",
    "                epoch_since_improved++\n",
    "\n",
    "                if epoch_since_improved >= 3:\n",
    "                    lr /= 10\n",
    "                \n",
    "            else:\n",
    "                epoch_since_improved = 0 \n",
    "                past_cost = current_cost\n",
    "\n",
    "            w, b = gradient_descent(x, y, b, w, epsilon, alpha, reg_bool)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #training set\n",
    "    x_tr = np.reshape(np.load(\"age_regression_Xtr.npy\"), (-1, 48*48))\n",
    "    y_tr = np.load(\"age_regression_ytr.npy\")\n",
    "\n",
    "    #testing set\n",
    "    x_te = np.reshape(np.load(\"age_regression_Xte.npy\"), (-1, 48*48))\n",
    "    y_te = np.load(\"age_regression_yte.npy\")\n",
    "\n",
    "    return x_tr, y_tr, x_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'boolean' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8137202c264b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_age_regressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse_bool\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_val\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mttv_bool\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mboolean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age_regression_Xtr.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mytr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age_regression_ytr.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age_regression_Xte.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boolean' is not defined"
     ]
    }
   ],
   "source": [
    "#Function Arguments:\n",
    "#mse_bool is the boolean value that determines if mse will be performed without or with regularization, default is True\n",
    "#alpha is the value for regularization, if used, default is 0.0\n",
    "#ttv_val is the value associated with the type of split wanted. A Train/Test Split is 0, and a train/validation/split is 1, No Split Needed is 2. Default is 2\n",
    "#learning rate is the hyperparameter associated with the gradient descent\n",
    "def train_age_regressor(mse_bool: bool = True, alpha: float = 0.0, ttv_val: int = 2, learning_rate: float, num_of_epochs: int):\n",
    "    # Load data\n",
    "    x_tr, y_tr, x_te, y_te = load_data()\n",
    "\n",
    "    if ttv_val == 0:\n",
    "        print(\"I made this before I realized train/test split is already given to you\")\n",
    "\n",
    "    elif ttv_val == 1:\n",
    "        x_val, y_val, x_te, y_te = random_test_validation_split(x_te, y_te, train_perc = 0.8)\n",
    "\n",
    "    #set intial w value \n",
    "    w = linear_regression(X_tr, ytr)\n",
    "\n",
    "    #set initial bias value \n",
    "    b = np.zeros(())\n",
    "\n",
    "    w_new, b_new = stochastic_gradient_descent(learning_rate = 0.01, num_of_epochs = 20, size_of_batch = 100, alpha= .01, n = len(y))\n",
    "    \n",
    "\n",
    "    if mse_bool == True:\n",
    "        train = mean_square_error(ytr, X_tr, w, b)\n",
    "        test = mean_square_error(yte, X_te, w, b)\n",
    "    else:\n",
    "        train = regularized_mean_square_error(ytr, X_tr, w, b_val, alpha)\n",
    "        test = regularized_mean_square_error(yte, X_te, w, b_val, alpha)\n",
    "\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "    # Report fMSE cost on the training and testing data (separately)\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(94.98633435331956, 358.60415394117115)\n"
     ]
    }
   ],
   "source": [
    "print(train_age_regressor(mse_bool=False, b_val=2, alpha=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}